####Experiments

#####Average
- model average score on the valid data set: 
```
[0.082, 0.1063, 0.0496, 0.0632, 0.0669, 0.0299, 0.0755, 0.0863, 0.0409, 0.0718]
67.24%!
```
As 07-09-2017
- model average (absb() changed to **2)
             precision    recall  f1-score   support

          0       0.92      0.87      0.90       991
          1       0.77      0.97      0.86      1064
          2       0.89      0.80      0.84       990
          3       0.77      0.80      0.79      1030
          4       0.83      0.82      0.83       983
          5       0.77      0.70      0.73       915
          6       0.91      0.90      0.91       967
          7       0.92      0.86      0.89      1090
          8       0.82      0.76      0.79      1009
          9       0.74      0.80      0.77       961

avg / total       0.83      0.83      0.83     10000

Accuracy: 83.07%!

As 11-09-2017

#####Contrast
- model contrast score on the valid data set:

Why is the score lower than the from Average? 
```
[0.082, 0.1063, 0.0496, 0.0632, 0.0669, 0.0299, 0.0755, 0.0863, 0.0409, 0.0718]
62.17%!
```
As 07-09-2017